{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f800c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train data set size \n",
      "(1460, 81)\n",
      "raw test data set size \n",
      "(1459, 80)\n",
      "before\n",
      "(1459, 66)\n",
      "training set size after cleaning \n",
      "(900, 66)\n",
      "Xtrain shape\n",
      "(900, 256)\n",
      "Xttest shape\n",
      "(1459, 256)\n",
      "Yrain shape\n",
      "(900,)\n",
      "(2359, 256)\n",
      "after\n",
      "(1459, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/14n26x893mj9929smn52_s840000gn/T/ipykernel_80324/3847779197.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  X = X.fillna(X.mean())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_raw = pd.read_csv(\"train.csv\")\n",
    "test_raw = pd.read_csv(\"test.csv\")\n",
    "print('raw train data set size ')\n",
    "print(train_raw.shape)\n",
    "print('raw test data set size ')\n",
    "print(test_raw.shape)\n",
    " \n",
    "X = pd.concat((train_raw.loc[:,'MSSubClass':'SaleCondition'], test_raw.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "Y = train_raw.SalePrice\n",
    "\n",
    "#Dropping irrelelvant attributes where almost 1300 rows are Null values\n",
    "X.drop(['GarageYrBlt','Alley','PoolQC','Fence','MiscFeature','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','BsmtFinSF2','LowQualFinSF','BsmtHalfBath'],axis=1,inplace=True)\n",
    "# fill the missing value automatically\n",
    "\n",
    "\n",
    "#Filling in with mean\n",
    "X['LotFrontage']=X['LotFrontage'].fillna(X['LotFrontage'].mean())\n",
    "\n",
    "#Filling in with mode\n",
    "X['BsmtCond']=X['BsmtCond'].fillna(X['BsmtCond'].mode()[0])\n",
    "X['BsmtQual']=X['BsmtQual'].fillna(X['BsmtQual'].mode()[0])\n",
    "X['FireplaceQu']=X['FireplaceQu'].fillna(X['FireplaceQu'].mode()[0])\n",
    "X['GarageType']=X['GarageType'].fillna(X['GarageType'].mode()[0])\n",
    "X['MasVnrType']=X['MasVnrType'].fillna(X['MasVnrType'].mode()[0])\n",
    "X['MasVnrArea']=X['MasVnrArea'].fillna(X['MasVnrArea'].mode()[0])\n",
    "\n",
    "X = X.fillna(X.mean()) \n",
    "\n",
    "# separet data back to remvoe outiers from training set\n",
    "train = X[:len(train_raw)]\n",
    "X_test = X[train_raw.shape[0]:]\n",
    "Y_train = Y\n",
    "print(\"before\")\n",
    "print(X_test.shape)\n",
    "\n",
    "# find the nomirecal attribute to identify outliers (creat df with number only)\n",
    "\n",
    "num_train = train.select_dtypes(include=[\"number\"])\n",
    "cat_train = train.select_dtypes(exclude=[\"number\"])\n",
    "\n",
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 = num_train.quantile(0.25)\n",
    "Q3 = num_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "idx = ~((num_train < (Q1 - 1.5 * IQR)) | (num_train > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "train_cleaned = pd.concat([num_train.loc[idx], cat_train.loc[idx]], axis=1)\n",
    "\n",
    "\n",
    "print('training set size after cleaning ')\n",
    "print(train_cleaned.shape)\n",
    "\n",
    "# convert categorical/discreate variables \n",
    "df = pd.concat([train_cleaned,X_test])\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "X_train = df[:len(train_cleaned)]\n",
    "X_test = df[train_cleaned.shape[0]:]\n",
    "Y_train = Y_train[idx]\n",
    "\n",
    "\n",
    "print('Xtrain shape')\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Xttest shape')\n",
    "print(X_test.shape)\n",
    "\n",
    "print('Yrain shape')\n",
    "print(Y_train.shape)\n",
    "print(df.shape)\n",
    "#rint(train)\n",
    "print(\"after\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebdcb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso training score (MSE)= 0.9446814539764274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulfattah/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.282e+10, tolerance: 2.995e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "name = \"lasso\"\n",
    "train_score = reg.score(X_train, Y_train)\n",
    "#val_score = reg.score(X_val, Y_val)\n",
    "\n",
    "print(name, \"training score (MSE)=\", train_score)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c16b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********How we can find parameters using GridSearchCV  for Regression***********\n",
      " Results from Grid Search \n",
      "\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingRegressor(learning_rate=0.01, max_depth=8, n_estimators=1500,\n",
      "                          subsample=0.2)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9139561076346692\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1500, 'subsample': 0.2}\n"
     ]
    }
   ],
   "source": [
    " print(format('How we can find parameters using GridSearchCV  for Regression','*^82'))\n",
    "\n",
    "    # loading libraries\n",
    "    #from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "    #Using GridSearchCV\n",
    "GBR = GradientBoostingRegressor()\n",
    "parameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n",
    "            'subsample'    : [0.9, 0.5, 0.2, 0.1],\n",
    "            'n_estimators' : [100,500,1000, 1500],\n",
    "            'max_depth'    : [4,6,8,10]\n",
    "             }\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, Y_train)\n",
    "\n",
    "    # Results from Grid Search\n",
    "print(\" Results from Grid Search \" )\n",
    "print()\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid_GBR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid_GBR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddaf64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11518726375649455\n"
     ]
    }
   ],
   "source": [
    "# import dependices \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# split data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train)\n",
    "\n",
    "# creat\n",
    "GBR = GradientBoostingRegressor(learning_rate = 0.03, max_depth = 8, n_estimators = 500, subsample = 0.2)\n",
    "# fit \n",
    "GBR.fit(X_train, Y_train)\n",
    "Y_predict = GBR.predict(X_val)\n",
    "Y_predict[Y_predict <= 0] = 1\n",
    "\n",
    "# check preformance \n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(np.log(Y_val), np.log(Y_predict))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "#predict\n",
    "Y_pred = GBR.predict(X_test)\n",
    "Y_pred[Y_pred <= 0] = 1\n",
    "pred_df = pd.DataFrame(Y_pred, index=test_raw[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('output_GBRGRid.csv', header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc4262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********How we can find parameters using GridSearchCV  for Regression***********\n",
      " Results from Grid Search \n",
      "\n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingRegressor(learning_rate=0.04, max_depth=4, n_estimators=500,\n",
      "                          subsample=0.2)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9041862151162734\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'subsample': 0.2, 'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.04}\n"
     ]
    }
   ],
   "source": [
    "print(format('How we can find parameters using GridSearchCV  for Regression','*^82'))\n",
    "\n",
    "    # loading libraries\n",
    "    #from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    # loading the diabetes datasets\n",
    "    #dataset = datasets.load_diabetes()\n",
    "    #X = dataset.data; y = dataset.target\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "    #Using GridSearchCV\n",
    "GBR = GradientBoostingRegressor()\n",
    "parameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n",
    "            'subsample'    : [0.9, 0.5, 0.2, 0.1],\n",
    "            'n_estimators' : [100,500,1000, 1500],\n",
    "            'max_depth'    : [4,6,8,10]\n",
    "             }\n",
    "grid_GBR = RandomizedSearchCV(GBR, parameters, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, Y_train)\n",
    "\n",
    "    # Results from Grid Search\n",
    "print(\" Results from Grid Search \" )\n",
    "print()\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid_GBR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid_GBR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3303917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09107069583953989\n"
     ]
    }
   ],
   "source": [
    "# import dependices \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# split data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train)\n",
    "\n",
    "# creat\n",
    "GBR = GradientBoostingRegressor(learning_rate = 0.01, max_depth = 4, n_estimators = 1500, subsample = 0.5)\n",
    "# fit \n",
    "GBR.fit(X_train, Y_train)\n",
    "Y_predict = GBR.predict(X_val)\n",
    "Y_predict[Y_predict <= 0] = 1\n",
    "\n",
    "# check preformance \n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(np.log(Y_val), np.log(Y_predict))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "#predict\n",
    "Y_pred = GBR.predict(X_test)\n",
    "Y_pred[Y_pred <= 0] = 1\n",
    "pred_df = pd.DataFrame(Y_pred, index=test_raw[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('output_GBR.csv', header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59def30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
